#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
æ”¯æŒå¤šGPUè®­ç»ƒçš„ä¾¿æ·å¯åŠ¨å™¨
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PanDerm-Guided Diffusionåˆ†å¸ƒå¼è®­ç»ƒ")
    
    # GPUç›¸å…³
    parser.add_argument(
        "--gpu_ids", 
        type=str, 
        default="0,1,2,3",
        help="GPUè®¾å¤‡IDï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ '0,1,2,3' æˆ– '0'"
    )
    parser.add_argument(
        "--num_processes", 
        type=int, 
        default=-1,
        help="è¿›ç¨‹æ•°ï¼Œ-1è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹GPUæ•°é‡"
    )
    
    # è®­ç»ƒå‚æ•°ï¼ˆä¼ é€’ç»™train.pyï¼‰
    parser.add_argument(
        "--epochs", 
        type=int, 
        default=50,
        help="è®­ç»ƒè½®æ•°"
    )
    parser.add_argument(
        "--batch_size", 
        type=int, 
        default=16,
        help="æ‰¹å¤§å°"
    )
    parser.add_argument(
        "--learning_rate", 
        type=float, 
        default=1e-4,
        help="å­¦ä¹ ç‡"
    )
    parser.add_argument(
        "--panderm_freeze", 
        action="store_true",
        help="å†»ç»“PanDermå‚æ•°"
    )
    parser.add_argument(
        "--mixed_precision", 
        action="store_true",
        default=True,
        help="å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ"
    )
    parser.add_argument(
        "--use_wandb", 
        action="store_true",
        help="å¯ç”¨WandBè®°å½•"
    )
    parser.add_argument(
        "--experiment_name", 
        type=str, 
        default="distributed-training",
        help="å®éªŒåç§°"
    )
    
    return parser.parse_args()


def check_gpu_availability(gpu_ids_str: str):
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    import torch
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ")
        return False, 0
    
    total_gpus = torch.cuda.device_count()
    print(f"âœ“ ç³»ç»Ÿæ€»å…±æœ‰ {total_gpus} ä¸ªGPU")
    
    if gpu_ids_str == "auto":
        gpu_ids = list(range(total_gpus))
    else:
        try:
            gpu_ids = [int(x.strip()) for x in gpu_ids_str.split(',')]
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„GPU IDæ ¼å¼: {gpu_ids_str}")
            return False, 0
    
    # æ£€æŸ¥GPU IDæ˜¯å¦æœ‰æ•ˆ
    invalid_ids = [gid for gid in gpu_ids if gid >= total_gpus]
    if invalid_ids:
        print(f"âŒ æ— æ•ˆçš„GPU ID: {invalid_ids}ï¼Œç³»ç»Ÿåªæœ‰ {total_gpus} ä¸ªGPU")
        return False, 0
    
    # æ£€æŸ¥GPUå†…å­˜
    for gpu_id in gpu_ids:
        gpu_name = torch.cuda.get_device_name(gpu_id)
        gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
        print(f"  GPU {gpu_id}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        if gpu_memory < 6.0:  # æœ€å°‘6GBå†…å­˜
            print(f"âš  GPU {gpu_id} å†…å­˜ä¸è¶³({gpu_memory:.1f}GB)ï¼Œå»ºè®®è‡³å°‘8GB")
    
    return True, len(gpu_ids)


def run_distributed_training(args):
    """è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ"""
    
    # æ£€æŸ¥GPU
    gpu_available, num_gpus = check_gpu_availability(args.gpu_ids)
    if not gpu_available:
        return False
    
    # è®¾ç½®è¿›ç¨‹æ•°
    if args.num_processes == -1:
        args.num_processes = num_gpus
    
    print(f"\n{'='*60}")
    print(f"å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ")
    print(f"{'='*60}")
    print(f"GPUè®¾å¤‡: {args.gpu_ids}")
    print(f"è¿›ç¨‹æ•°: {args.num_processes}")
    print(f"æ‰¹å¤§å°: {args.batch_size}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"æ··åˆç²¾åº¦: {args.mixed_precision}")
    print(f"{'='*60}\n")
    
    # æ„å»ºaccelerate launchå‘½ä»¤
    cmd = [
        "accelerate", "launch",
        "--num_processes", str(args.num_processes),
        "--multi_gpu",
        "--mixed_precision", "fp16" if args.mixed_precision else "no",
    ]
    
    # æ·»åŠ è®­ç»ƒè„šæœ¬å’Œå‚æ•°
    train_script = str(project_root / "scripts" / "train.py")
    cmd.extend([
        train_script,
        "--epochs", str(args.epochs),
        "--batch_size", str(args.batch_size),
        "--learning_rate", str(args.learning_rate),
        "--gpu_ids", args.gpu_ids,
        "--num_processes", str(args.num_processes),
        "--distributed",
        "--experiment_name", f"{args.experiment_name}_distributed",
    ])
    
    # å¯é€‰å‚æ•°
    if args.panderm_freeze:
        cmd.append("--panderm_freeze")
    if args.mixed_precision:
        cmd.append("--mixed_precision")
    if args.use_wandb:
        cmd.append("--use_wandb")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = args.gpu_ids
    env["PYTHONPATH"] = f"{project_root}:{env.get('PYTHONPATH', '')}"
    
    print("æ‰§è¡Œå‘½ä»¤:")
    print(" ".join(cmd))
    print()
    
    try:
        # è¿è¡Œè®­ç»ƒ
        result = subprocess.run(cmd, env=env, check=True)
        print("\nğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : {e.returncode}")
        return False
    except KeyboardInterrupt:
        print("\nâš  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¼‚å¸¸: {e}")
        return False


def check_accelerate():
    """æ£€æŸ¥accelerateæ˜¯å¦å®‰è£…"""
    try:
        import accelerate
        print(f"âœ“ Accelerateç‰ˆæœ¬: {accelerate.__version__}")
        
        # æ£€æŸ¥accelerateé…ç½®
        result = subprocess.run(
            ["accelerate", "env"], 
            capture_output=True, 
            text=True, 
            timeout=10
        )
        
        if result.returncode == 0:
            print("âœ“ Accelerateç¯å¢ƒé…ç½®æ­£å¸¸")
            return True
        else:
            print("âš  Accelerateç¯å¢ƒé…ç½®å¼‚å¸¸ï¼Œè¯·è¿è¡Œ: accelerate config")
            return False
            
    except ImportError:
        print("âŒ æœªå®‰è£…accelerateï¼Œè¯·è¿è¡Œ: pip install accelerate")
        return False
    except subprocess.TimeoutExpired:
        print("âš  Accelerateç¯å¢ƒæ£€æŸ¥è¶…æ—¶")
        return True
    except Exception as e:
        print(f"âš  Accelerateæ£€æŸ¥å¼‚å¸¸: {e}")
        return True


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("PanDerm-Guided Diffusion åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨")
    print("="*60)
    
    # è§£æå‚æ•°
    args = parse_args()
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("\næ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥accelerate
    if not check_accelerate():
        print("\nè¯·å…ˆé…ç½®accelerateç¯å¢ƒ:")
        print("  pip install accelerate")
        print("  accelerate config")
        return False
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_env = os.environ.get("CONDA_DEFAULT_ENV")
    if conda_env:
        print(f"âœ“ å½“å‰condaç¯å¢ƒ: {conda_env}")
    else:
        print("âš  æœªæ£€æµ‹åˆ°condaç¯å¢ƒ")
    
    # è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    success = run_distributed_training(args)
    
    if success:
        print("\n" + "="*60)
        print("è®­ç»ƒå®Œæˆï¼æ£€æŸ¥ç»“æœ:")
        print("- æ£€æŸ¥ç‚¹: ./checkpoints/")
        print("- æ—¥å¿—: ./logs/")  
        print("- ç”Ÿæˆå›¾åƒ: ./outputs/")
        if args.use_wandb:
            print("- WandBé¢æ¿: https://wandb.ai")
        print("="*60)
    
    return success


if __name__ == "__main__":
    sys.exit(0 if main() else 1)